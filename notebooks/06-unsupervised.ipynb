{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "149092e9-7aed-4e97-9cc6-c370d6ff7ede",
   "metadata": {},
   "source": [
    "# Aprendizagem não supervisionada\n",
    "\n",
    "A aprendizagem não supervisionada é uma área da aprendizagem automática que se foca na extração de padrões e estruturas interessantes de dados não etiquetados. Em Python, a biblioteca [scikit-learn](https://scikit-learn.org/) é uma das ferramentas mais utilizadas para aplicar abordagens de aprendizagem não supervisionada e de aprendizagem automática em geral. Com uma vasta coleção de algoritmos de agrupamento (*clustering*), redução de dimensionalidade e deteção de anomalias, a biblioteca *scikit-learn* oferece uma ampla gama de funcionalidades para explorar e compreender conjuntos de dados não etiquetados. Este tutorial foca-se maioritariamente na aplicação de algumas abordagens de *clustering*. Para obter informação sobre outras funcionalidades disponibilizadas pela biblioteca *scikit-learn*, recomendamos a consulta da sua [documentação](https://scikit-learn.org/stable/user_guide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352fdd91-7d60-4cd1-ae96-f4647068b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f86e187-ef0d-4932-8261-40b55dd631fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_theme() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a953196-0606-4fea-89f8-b9072f83a0cf",
   "metadata": {},
   "source": [
    "## Manipulação de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cec085-0899-4946-bdb7-bb89b4a2e8a8",
   "metadata": {},
   "source": [
    "Para além de implementações eficientes de algoritmos de aprendizagem automática, a biblioteca *scikit-learn* também disponibiliza algumas funcionalidades de manipulação de dados relevantes no contexto da aplicação desses algoritmos. Por exemplo, normalização e redução de dimensionalidade. Para demonstrar algumas destas funcionalidades, vamos usar o conjunto de dados [Iris](https://archive.ics.uci.edu/dataset/53/iris) que já vimos anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af8e364e-55e3-4e1a-87dd-a592db344dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = sns.load_dataset('iris')\n",
    "iris.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad1c31a-217a-457a-83b4-e672f7c68a54",
   "metadata": {},
   "source": [
    "**Nota**: O método `sample` escolhe aleatoriamente uma amostra de *n* exemplos do conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8684c79-838c-400e-a904-1c23d0740c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris, hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1fad8-77e0-4105-b03b-afcbca867e20",
   "metadata": {},
   "source": [
    "Num contexto de aprendizagem não supervisionada, a espécie de planta serve apenas como uma indicação de uma possível separação dos dados que tem um significado conhecido no mundo real. Para além disso, mesmo que estivessemos num contexto de aprendizagem supervisionada, a biblioteca *scikit-learn* lida separadamente com os atributos usados para representar os dados e aquele ou aqueles que representam o que se pretende prever. Como tal, para simplificar a aplicação das funções da biblioteca, vamos criar uma nova tabela de dados sem o atributo `species`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e63b90-1e85-4e0e-aa30-05a1cd6cee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features = iris.drop(columns=['species'])\n",
    "iris_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2d3f4-6c24-4315-ba15-8eacb742c3eb",
   "metadata": {},
   "source": [
    "**Nota**: A subdivisão dos exemplos por espécie de planta vai continuar a ser usada como apoio em algumas visualizações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f5e59-1fd0-476f-907f-74ec5ae379aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalização\n",
    "\n",
    "A normalização dos valores dos atributos pode ter um impacto relevante quando estes têm escalas diferentes. A classe `StandardScaler` da biblioteca *scikit-learn* pode ser usada para fazer normalização baseada em *z-score*, isto é, transformar os atributos de forma a que a distribuição dos seus valores tenha média zero e desvio padrão unitário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69b5015-e6cf-4614-bbd8-83e200d73fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e1b70ca-76b6-40a3-9e5a-1b72cda094b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "std_scaler.fit(iris_features)\n",
    "scaled_features = std_scaler.transform(iris_features)\n",
    "std_scaled_iris = pd.DataFrame(scaled_features, index=iris_features.index, columns=iris_features.columns)\n",
    "std_scaled_iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f0c392-91f1-48e2-b4f5-55c253c0914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(std_scaled_iris);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2103d-e0bb-43a4-8d6c-175e3522a76b",
   "metadata": {},
   "source": [
    "**Nota**: O método `fit` é usado para ajustar o normalizador, isto é, extrair dos dados a informação necessária para fazer a normalização (neste caso, a média e o desvio padrão de cada atributo), enquanto o método `transform` é usado para aplicar a normalização. O método `transform` pode ser aplicado sobre exemplos diferentes daqueles que foram usados para ajustar o normalizador. Isto permite normalizar novos dados da mesma forma que os do conjunto de dados original. O método `fit_transform` pode ser usado para efetuar as duas operações sequencialmente sobre o mesmo conjunto de dados, o que é conveniente em muitos casos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fba1593-f55f-4a34-a1ec-e0009bbe46cb",
   "metadata": {},
   "source": [
    "Outra abordagem de normalização é escalar os valores dos atributos de forma a que estes fiquem dentro de um determinado intervalo, como por exemplo entre 0 e 1. Isso pode ser feito usando a funcionalidade da classe `MinMaxScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68212c18-8835-4364-995c-dd517206f5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2343a4-3a91-4581-a212-1b635dd7f57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler((0, 1))\n",
    "minmax_scaled_iris = pd.DataFrame(minmax_scaler.fit_transform(iris_features), index=iris_features.index, columns=iris_features.columns)\n",
    "minmax_scaled_iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f676bbc7-2a48-4371-9878-df6f2bc861c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(minmax_scaled_iris);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd902ad9-585e-457f-be7d-9773d10b5101",
   "metadata": {},
   "source": [
    "### Redução de dimensionalidade\n",
    "\n",
    "Muitas vezes é necessário reduzir a dimensionalidade de um conjunto de dados para o conseguir analisar, visualizar, ou até mesmo para melhorar o desempenho de modelos de aprendizagem automática obtidos a partir desses dados. Existem várias abordagens para reduzir a dimensionalidade, sendo as mais comuns baseadas na seleção de um subconjunto representativo dos exemplos e/ou atributos originais de acordo com um determinado critério. Outra abordagem possível é fazer uma análise de componentes principais ([PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)) e usar como atributos os componentes que refletem a maior variância no conjunto de dados. Para isso, pode ser usada a classe `PCA` da biblioteca *scikit-learn*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f669faca-dd83-4e04-98df-4dcb02eef306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b595e2c0-6ca3-41a1-aa61-e668bf30fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(iris_features)\n",
    "pca_iris = pd.DataFrame(pca.transform(iris_features), index=iris_features.index, columns=['PC1', 'PC2'])\n",
    "pca_iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6ce8ca-124b-4919-9bad-c3a0a36cef5e",
   "metadata": {},
   "source": [
    "**Nota**: Neste caso, escolhemos os dois componentes principais, o que nos ajuda a visualizar o conjunto de dados. Esta é uma abordagem comum, mesmo em casos em que os atributos originais são usados para fazer a restante análise do conjunto de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bae42d7-4ed9-48da-a39d-8c5e020e5cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(pca_iris, x='PC1', y='PC2', hue=iris['species']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3770843-8ba4-4262-9f1e-729e0632ebbd",
   "metadata": {},
   "source": [
    "Para verificar se os componentes selecionados são suficientemente representativos do conjunto de dados, podemos olhar para o rácio da variância total no conjunto de dados que estes explicam. Esta informação é dada pelo atributo `explained_variance_ratio_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a731463-d22b-4fae-8a79-4fdac4bc14db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)\n",
    "print(pca.explained_variance_ratio_.sum() * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1587f57a-1453-4eab-81d5-ab4e6b62b268",
   "metadata": {},
   "source": [
    "Neste caso, podemos verificar que os dois componentes explicam perto de 98% da variância total no conjunto de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e29559-de18-4bb3-a824-2105e7ba7985",
   "metadata": {},
   "source": [
    "## Agrupamento (*Clustering*)\n",
    "\n",
    "O agrupamento, mais conhecido pelo termo inglês *clustering*, é uma técnica de aprendizagem não supervisionada que consiste em agrupar um conjunto de dados em grupos (clusters) em que os elementos de cada grupo são mais semelhantes entre si do que aos elementos de outros grupos. A biblioteca *scikit-learn* implementa vários algoritmos de *clustering*. Vamos explorar três: [k-Means](#k-Means), [agrupamento hierárquico aglomerativo](#Agrupamento-hierárquico-aglomerativo) e [DBSCAN](#DBSCAN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e71b71-6554-49dc-9d6c-508658ec40cc",
   "metadata": {},
   "source": [
    "### k-Means\n",
    "\n",
    "O algoritmo [k-Means](https://en.wikipedia.org/wiki/K-means_clustering) agrupa os exemplos de um conjunto de dados de forma a que estes fiquem separados em *k* clusters de igual variância, minimizando o valor da soma dos quadrados das distâncias de cada exemplo ao centroide do seu cluster (*within-cluster sum-of-squares*). O algoritmo requer que o número de clusters, *k*, seja específicado. Esta abordagem de *clustering* escala bem para conjuntos de dados com muitos exemplos e é a mais usada no contexto da aprendizagem não supervisionada.\n",
    "\n",
    "Na biblioteca *scikit-learn*, o algortimo *k-Means* é implementado pela classe `KMeans`, sendo o número de clusters definido pelo argumento `n_clusters`. Neste caso, vamos começar por usar um valor de *k=3* para comparação com a classificação das plantas por espécie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0da837b6-85a9-427c-aea1-24cc150ebe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fce02e91-9df7-4dc0-a220-19d20ca3d0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, n_init='auto').fit(iris_features) # The n_init='auto' is only explicited to avoid a warning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b03818-9c15-48f0-8cf5-5056eaef0af1",
   "metadata": {},
   "source": [
    "**Nota**: A implementação do algoritmo k-Means na biblioteca *scikit-learn* inclui algumas otimizações. Por exemplo, a forma predefinida para escolha dos centroides iniciais segue o algoritmo [k-means++](https://en.wikipedia.org/wiki/K-means%2B%2B). Para além disso, esta implementação só permite o uso da distância euclidiana como função de distância."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b23c856-33e8-4a17-b15f-74f8caad68bc",
   "metadata": {},
   "source": [
    "Os centroides dos clusters obtidos e as atribuições dos exemplos aos clusters são dados pelos atributos `cluster_centers_` e `labels_`, respetivamente: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2051e153-edb8-419d-9ea0-0c07fad79766",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3e1d72e4-d784-4143-9937-0565286f341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60255d-301f-485c-ade8-8b5f6ba1f257",
   "metadata": {},
   "source": [
    "Vamos guardar este agrupamento na tabela de dados para podermos visualizá-lo e compará-lo com os obtidos usando os outros algoritmos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3ec3ab89-5fc9-4e69-b1ec-217e2b89db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['kmeans'] = pd.Categorical(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f92aa495-d26d-40eb-b054-04d20ec5173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris, hue='kmeans');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89804822-180c-4a45-b542-4cf4b9c1767c",
   "metadata": {},
   "source": [
    "Também podemos visualizar o agrupamento usando a projeção dos dados nos dois componentes principais obtidos através da aplicação de [PCA](#Redução-de-dimensionalidade):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e4011154-a59b-41d3-b816-29e0dafdf5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(pca_iris, x='PC1', y='PC2', hue=iris['kmeans']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc15d62-208b-4e04-8a5c-0e9ad4d7fa67",
   "metadata": {},
   "source": [
    "Podemos ver que o agrupamento obtido é distinto da classificação das plantas por espécie, mas tem algumas semelhanças:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b8822198-39b3-4892-8389-0af384880aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(pca_iris, x='PC1', y='PC2', hue=iris['species']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543ce4a-577e-4b14-b0b3-2592e704452f",
   "metadata": {},
   "source": [
    "O atributo `inertia_` guarda o valor da soma dos quadrados das distâncias de cada exemplo ao centroide do seu cluster (*within-cluster sum of squares*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ad5fcca4-4f2f-415a-85a0-f88143f9ec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd31a8c3-339b-43c3-8ea5-46d7c868c9d8",
   "metadata": {},
   "source": [
    "Podemos usar esta informação para analisar a qualidade do agrupamento para vários valores de *k*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac561a9c-9b1d-44a5-ac97-fddf3359dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_wss = pd.DataFrame([(k, KMeans(n_clusters=k, n_init='auto').fit(iris_features).inertia_) for k in range(1, 11)], columns=['k', 'wss'])\n",
    "sns.lineplot(k_wss, x='k', y='wss');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb737dc3-a139-4593-964f-331dc6ad152a",
   "metadata": {},
   "source": [
    "Usando o método do \"cotovelo\" podemos concluir que *k=3* é um valor adequado para o número de clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c14ee2-09de-497c-8e0c-760af1c7d3cd",
   "metadata": {},
   "source": [
    "### Agrupamento hierárquico aglomerativo\n",
    "\n",
    "O [clustering hierárquico](https://en.wikipedia.org/wiki/Hierarchical_clustering) é uma técnica de agrupamento que constrói clusters aninhados, unindo-os ou dividindo-os sucessivamente. A hierarquia de clusters pode ser representada como uma árvore (ou dendrograma). A raiz da árvore é o cluster único que reúne todos os exemplos e as folhas são os clusters com apenas um exemplo. O agrupamento hierárquico aglomerativo usa uma abordagem de baixo para cima, isto é, cada exemplo começa como um cluster e, a cada iteração do algoritmo, um par de clusters é unido. Na biblioteca *scikit-learn*, este algoritmo é implementado pela classe `AgglomerativeClustering`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b5bfbd9d-642b-4d49-bfeb-edd760d0ed35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "42c3473d-7b76-4b18-b522-e339f1787fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical = AgglomerativeClustering(n_clusters=3).fit(iris_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a0f71-765b-49f8-a25e-8104aeee3932",
   "metadata": {},
   "source": [
    "**Nota**: Neste caso, o argumento `n_clusters` é usado para definir o número de clusters que se pretende obter no final, evitando a construção da árvore completa. Mais à frente vamos ver como construir e visualizar toda a árvore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b3c50-8eae-4587-aad9-2922eeacdaa2",
   "metadata": {},
   "source": [
    "Tal como no caso do algoritmo *k-Means*, o atributo `labels_` guarda as atribuições dos exemplos aos clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "24e8c556-6b44-4033-b888-c995ad820b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb5097-141b-4195-afab-8fa688656755",
   "metadata": {},
   "source": [
    "Mais uma vez, vamos guardar o agrupamento na tabela de dados para visualização e comparação com os restantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "afca0e75-4b2b-49fe-92df-7df56d8a17b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['hierarchical'] = pd.Categorical(hierarchical.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "24d728c7-63fd-4452-b25c-1a82fae14a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris, hue='hierarchical');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "40a24c10-2825-4702-9520-259379bbe04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(pca_iris, x='PC1', y='PC2', hue=iris['hierarchical']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf22b813-d069-4df2-bbf8-a3abc41e9901",
   "metadata": {},
   "source": [
    "Neste caso, o agrupamento obtido é muito semelhante ao obtido usando *k-means*, apenas com alguma variação na fronteira entre os dois *clusters* à direita. Esta diferença deve-se à sequência de otimizações locais efetuada pelo algoritmo de agrupamento hierárquico em contraste com a otimização global feita pelo algoritmo *k-means*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1188a6b5-80c8-4235-9c12-f0c588a03fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(pca_iris, x='PC1', y='PC2', hue=iris['kmeans']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e3ab9b-c616-4601-b49b-159ecd8d5d02",
   "metadata": {},
   "source": [
    "A forma como a distância entre dois *clusters* é calculada pode ser definida usando o argumento `linkage`. Por predefinição, é usado o [método de Ward](https://en.wikipedia.org/wiki/Ward%27s_method). Outras opções são `'average'`, `'single'` e `'complete'`. Por exemplo, usar a distância mínima entre pontos dos dois *clusters* (`'single'`) leva a resultados significativamente diferentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "feffd490-03d7-4b6b-abc3-9486c7a1eb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_single_link = AgglomerativeClustering(n_clusters=3, linkage='single').fit(iris_features)\n",
    "sns.scatterplot(pca_iris, x='PC1', y='PC2', hue=hierarchical_single_link.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cda8c-93a9-4508-8316-f45c31cc0fbc",
   "metadata": {},
   "source": [
    "Para obter a árvore completa, podemos aplicar o algoritmo com o argumento `n_clusters=None` e o argumento `distance_threshold=0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9320ac68-a199-448e-a7be-dafbd2f71199",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_full = AgglomerativeClustering(n_clusters=None, distance_threshold=0).fit(iris_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691d35ee-8cee-464f-97ed-533636033b6e",
   "metadata": {},
   "source": [
    "Para visualizar a árvore como um dendrograma podemos usar a seguinte função auxiliar definida na [documentação](https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html) da biblioteca *scikit-learn*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b33793a1-278b-4ab7-9eea-6eba048cb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b91cbdc2-d37f-4a68-9e39-c415fd279acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(hierarchical_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f755bb-d6e9-4929-9fd5-545189cb27f6",
   "metadata": {},
   "source": [
    "**Nota**: A visualização da árvore completa torna-se confusa para um número grande de exemplos. O argumento `truncate_mode` pode ser usado para cortar a árvore para incluir apenas um número limitado de clusters (`'lastp'`) ou ir apenas até um determinado nível de profundidade (`'level'`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "89866e0b-6309-469d-bb25-45126d304c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(hierarchical_full, truncate_mode='lastp', p=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d8da9c0f-bd7b-4c00-9e53-d503880537f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(hierarchical_full, truncate_mode='level', p=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82b1cc-0789-4791-b0e9-0a6d9cbc7151",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "O algoritmo [DBSCAN](https://en.wikipedia.org/wiki/DBSCAN) encontra clusters de exemplos com base na densidade. Isto é, o algoritmo considera clusters como sendo áreas de alta densidade separadas por áreas de baixa densidade. Isto permite que que os clusters encontrados possam ter qualquer forma. Para além disso, não é necessário definir à priori o número de clusters a encontrar. Por outro lado, é necessário definir os parâmetros que controlam a densidade necessária para formar um cluster: o raio (`eps`) e o número de pontos dentro desse raio (`min_samples`). Na biblioteca *scikit-learn*, o algoritmo *DBSCAN* é implementado pela classe `DBSCAN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2b4f4166-9365-4e83-a1ac-af90e810b6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5290012d-d44a-462e-89ac-c39f307dfb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.5, min_samples=5).fit(iris_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67c4ad0-5c71-4d33-a287-ccfc57c1caf2",
   "metadata": {},
   "source": [
    "Neste caso, a atribuição dos exemplos aos clusters dada pelo atributo `labels_` também pode incluir o valor `-1` que indica que o exemplo não faz parte de nenhum cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e6a45996-291f-4048-98b3-97e16a3ab142",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26a2792-93b7-4469-a5e4-59f3e48a5627",
   "metadata": {},
   "source": [
    "Mais uma vez, vamos guardar e visualizar o agrupamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "32476a15-b404-4a5a-b7e7-6bba602efb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris['dbscan'] = pd.Categorical(dbscan.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4817cf0f-57a7-4933-bb40-17e02f6eaf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(iris, hue='dbscan');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "78c10450-256c-4a70-a836-2b03088a737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(pca_iris, x='PC1', y='PC2', hue=iris['dbscan']);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1884da-f379-4ff8-afe4-f3a0e841ac8c",
   "metadata": {},
   "source": [
    "Neste caso, podemos ver que o agrupamento consiste em apenas dois clusters. Para além disso, existe um conjunto de pontos que não fazem parte de nenhum dos clusters e podem ser considerados anomalias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9b366d-652d-4b0f-b77b-d3bc6c79276b",
   "metadata": {},
   "source": [
    "Até agora tem sido usada a distância euclideana em todas as abordagens de agrupamento. No entanto, a implementação do algoritmo *DBSCAN* na biblioteca *scikit-learn* permite o uso de outras funções de distância através do argumento `metric`. Por exemplo, podemos usar a distância de Manhattan, o que leva a resultados bastante diferentes se os restantes parâmetros não forem alterados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5abd08ac-6ff5-4b82-8066-c41294744889",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_manhattan = DBSCAN(eps=0.5, min_samples=5, metric='manhattan').fit(iris_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3b40813a-f40b-459b-a953-268816932eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(pca_iris, x='PC1', y='PC2', hue=dbscan_manhattan.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7285122b-bac9-4f97-8d52-307bfaad2426",
   "metadata": {},
   "source": [
    "**Nota**: A biblioteca *scikit-learn* disponibiliza múltiplas [funções de distância](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html). Cada uma dessas funções tem características diferentes. Como tal, os restantes parâmetros devem ser ajustados de forma adequada e a função de distância deve ser escolhida tendo em conta as características dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec9d60-2433-4b18-b263-29802b7d8bd2",
   "metadata": {},
   "source": [
    "**Nota**: Também é possível variar a função de distância usada pelo algoritmo de agrupamento hierárquico aglomerativo, desde que não seja usado o método de Ward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ca47a7-b5de-48be-ac29-981a19333b53",
   "metadata": {},
   "source": [
    "### Avaliação\n",
    "\n",
    "A biblioteca *scikit-learn* disponibiliza um conjunto de métricas para avaliar a qualidade de agrupamentos sem depender de etiquetas externas, como por exemplo o [coeficiente de silhueta](#Coeficiente-de-silhueta), o [índice Calinski-Harabasz](#Índice-Calinski-Harabasz) e o [índice Davies-Bouldin](#Índice-Davies-Bouldin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b552ca41-cd35-4517-a14f-fdecb49ffd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56578062-bc3a-42d5-be66-2238b47e6d34",
   "metadata": {},
   "source": [
    "#### Coeficiente de silhueta\n",
    "\n",
    "O [coeficiente de silhueta](https://en.wikipedia.org/wiki/Silhouette_(clustering)) é uma medida de quão similar um exemplo é ao aos outros exemplos do seu cluster (coesão) em comparação com os exemplos do cluster vizinho mais próximo (separação). O coeficiente de silhueta varia entre -1 e 1, sendo que um valor alto indica que o exemplo está bem enquadrado no seu próprio cluster. A média dos valores da silhueta para todos os pontos de um conjunto de dados é um bom indicador da qualidade de um agrupamento. Este valor pode ser calculado usando a função `silhouette_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "937748ee-a6a8-4cab-ab89-2a5af628ec82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['kmeans', 'hierarchical', 'dbscan']:\n",
    "    print(f'{c}: {sklearn.metrics.silhouette_score(iris_features, iris[c])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6477476-9c3c-49ee-9ce4-8bc914c3d719",
   "metadata": {},
   "source": [
    "#### Índice Calinski-Harabasz\n",
    "\n",
    "O [índice Calinski-Harabasz](https://en.wikipedia.org/wiki/Calinski-Harabasz_index), também conhecido como *Variance Ratio Criterion*, é o rácio entre a separação entre clusters (*between-cluster sum of squares*) e a dispersão dentro dos clusters (*within-cluster sum of squares*). Valores mais altos indicam um melhor agrupamento. Este índice pode ser calculado usando a função `calinski_harabasz_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "348ed876-521a-4111-82a9-955e8cf17d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['kmeans', 'hierarchical', 'dbscan']:\n",
    "    print(f'{c}: {sklearn.metrics.calinski_harabasz_score(iris_features, iris[c])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24da458-c9e2-46b6-9723-836abca5eae4",
   "metadata": {},
   "source": [
    "#### Índice Davies-Bouldin\n",
    "\n",
    "O [índice Davies-Bouldin](https://en.wikipedia.org/wiki/Davies-Bouldin_index) indica a similaridade média entre clusters, em que a similaridade é uma medida que compara a distância entre clusters com o tamanho desses mesmos clusters. O valor minímo deste índice é zero, sendo que valores mais baixos indicam um melhor agrupamento. Este índice pode ser calculado usando a função `davies_bouldin_score`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "070ba33a-669e-4ff4-8b1d-be5282cfc234",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in ['kmeans', 'hierarchical', 'dbscan']:\n",
    "    print(f'{c}: {sklearn.metrics.davies_bouldin_score(iris_features, iris[c])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef2e05-1d6b-4e35-a80c-9b324a34f324",
   "metadata": {},
   "source": [
    "### Exemplos adicionais\n",
    "\n",
    "Vamos explorar alguns exemplos adicionais para identificar situações em que as diferentes abordagens de *clustering* levam a resultados distintos. Para isso, vamos usar as funcionalidades de geração de conjuntos de dados da biblioteca *scikit-learn*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1b0fac80-d0f0-45b2-a683-ab0dec82031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735fa987-5e22-4b31-9300-018533f15934",
   "metadata": {},
   "source": [
    "Por exemplo, vamos criar um conjunto de dados com dois grupos distintos não circulares usando a função `make_moons`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "662b7047-7c22-4b7e-9222-f744ca73baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "moons = pd.DataFrame(sklearn.datasets.make_moons(n_samples=1000, noise=0.05)[0], columns=['x', 'y'])\n",
    "sns.scatterplot(moons, x='x', y='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a65587-aeb6-4fc5-b8c6-b09dc24f98ef",
   "metadata": {},
   "source": [
    "Vamos aplicar as três abordagens de *clustering* que vimos anteriormente sobre este conjunto de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "87d3ad4e-b7a3-4904-a571-6351d8802e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "moons_clustering = {\n",
    "    'kmeans': KMeans(n_clusters=2, n_init='auto').fit(moons).labels_,\n",
    "    'hierarchical': AgglomerativeClustering(n_clusters=2).fit(moons).labels_,\n",
    "    'dbscan': DBSCAN(eps=0.25, min_samples=5).fit(moons).labels_\n",
    "}\n",
    "\n",
    "for m, c in moons_clustering.items():\n",
    "    sns.scatterplot(moons, x='x', y='y', hue=c).set(title=m)\n",
    "    plt.show() # This is used to force every plot to be shown instead of just the last one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcb8dc9-169a-4c8a-a597-6999f0535996",
   "metadata": {},
   "source": [
    "Neste caso, podemos observar resultados bastantes diferentes para as três abordagens. Apenas o agrupamento obtido usando DBSCAN corresponde ao esperado, uma vez que é baseado na densidade de exemplos num determinado espaço. Por outro lado, o algoritmo *k-means* e o agrupamento hierárquico usando o método de Ward levam a agrupamentos circulares distintos devido às diferentes estratégias de otimização."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7af62b-4a8f-4b8b-ba53-207904ac9b84",
   "metadata": {},
   "source": [
    "**Nota**: É possível obter o agrupamento esperado usando a abordagem de agrupamento hierárquico se for usada a distância mínima entre pontos dos dois clusters em vez do método de Ward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fcdaea1b-09de-412e-bee2-d5cc792f2fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "moons_single = AgglomerativeClustering(n_clusters=2, linkage='single').fit(moons)\n",
    "sns.scatterplot(moons, x='x', y='y', hue=moons_single.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b7da4b-629b-4472-bddc-72d54346157c",
   "metadata": {},
   "source": [
    "Agora vamos usar a função `make_blobs` para criar um conjunto de dados com três grupos circulares, dois deles muito próximos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0604d4ba-c477-4a40-8284-de493e108d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs = pd.DataFrame(sklearn.datasets.make_blobs(n_samples=1000, centers=3, random_state=1000)[0], columns=['x', 'y'])\n",
    "sns.scatterplot(blobs, x='x', y='y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f241057-6cfd-4d89-8ad6-12a9e15e8df3",
   "metadata": {},
   "source": [
    "**Nota**: O argumento `random_state` é usado para que o resultado seja reprodutível. Este pode ser usado na maioria das funções definidas pela biblioteca *scikit-learn* que envolvem algum tipo de aleatoriedade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f13785ef-6999-4529-9cd6-74014dba0c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_clustering = {\n",
    "    'kmeans': KMeans(n_clusters=3, n_init='auto').fit(blobs).labels_,\n",
    "    'hierarchical': AgglomerativeClustering(n_clusters=3).fit(blobs).labels_,\n",
    "    'dbscan': DBSCAN(eps=0.5, min_samples=5).fit(blobs).labels_\n",
    "}\n",
    "\n",
    "for m, c in blobs_clustering.items():\n",
    "    sns.scatterplot(blobs, x='x', y='y', hue=c).set(title=m)\n",
    "    plt.show() # This is used to force every plot to be shown instead of just the last one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed489b0-d3b9-4d82-8ba5-f91b2cb35133",
   "metadata": {},
   "source": [
    "Neste caso, podemos ver que, tal como no conjunto de dados *Iris*, os agrupamentos gerados pelo algoritmo *k-Means* e pelo agrupamento hierárquico aglomerativo são muito semelhantes, havendo apenas pequenas variações nas fronteiras entre clusters. Por outro lado, o agrupamento gerado pelo algoritmo *DBSCAN* é bastante diferente, juntando os exemplos dos dois *blobs* mais próximos e separando um pequeno grupo de exemplos dos restantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac41f8-eff6-4575-8b43-9ee640649b75",
   "metadata": {},
   "source": [
    "**Nota**: Não esquecer que variações dos parâmetros de controlo levam a resultados diferentes. Por isso, devem ser exploradas múltiplas configurações."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
